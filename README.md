# CS 4100 Final Project Report
## An Artificial-Intelligence Approach To Exploding Kittens
### Gabe Holmes & Payton McAlice

# Introduction 
## Motivation
Exploding Kittens is a turn-based card game that has recently gained a lot of popularity because of its interesting strategy and fun presentation. At the end of a player’s turn, they will have to draw from a deck of cards with “Exploding Kittens” in them. If you draw one of these cards, you are eliminated from the game. The goal is to be the last player standing. Along the way, players can choose to play cards during their turn to gain an advantage. Some of these cards include: <br>
* Attack, which allows a user to forgo drawing from the deck and in return making the next player draw twice
* Skip, which allows a user to completely skip one draw from the deck
* See The Future, which allows a user to see the top three cards in the deck
* Shuffle, which allows the user to shuffle the deck randomly
* Favor, which allows a user to ask for a card from the hand of another player
* Cat cards, such as Tacocat, Cattermelon, and Rainbow-Ralphing Cat, which are useless alone but when played with like cards can be used to steal a card from any opponent <br>

The arguably most valuable card is the Defuse card, which is automatically played if a holder draws an Exploding Kitten from the deck. The card essentially gives the player an extra life, allowing them to reinsert the Exploding Kitten wherever into the deck that they please. <br>
Of course within the game, there are plenty of moves you can make during your turn to increase your chances of survival or decrease your opponents’ chances of survival. Different plays can put you on the offensive or defensive, and usually the best choices are based on your limited knowledge of the game; this knowledge may include what is in the deck, what is in the hands of your opponents, and any information an opponent may have about your hand. <br>
As a fairly new card game with very few online or computerized implementations, there is not a lot of precedent for artificial intelligence in Exploding Kittens. In spite of this, the game in itself is a rather compelling artificial intelligence problem. The game-states are extremely complex and a player’s knowledge of the state is almost always and certainly incomplete. That being said, the optimal moves are often those that are influenced by a player’s understanding of the game-state; knowing what cards are where greatly affects your short-term and possibly long-term strategy. Additionally, players during their turn hold a lot of agency over the game-state.  During a turn, a player can make as many moves as they want so long as they have the cards to make said moves. <br>
The motivation of this project is to begin the implementation of a complex artificial intelligence model that can competitively play Exploding Kittens. Our interest comes from the complexity of the game and its strategy, as well as the relative lack of precedent in this area. <br>
## Challenges
Even at the surface, there are immediate challenges that are faced when considering the problem at hand. Most of these challenges are in relation to the representation of the game and its states. As previously mentioned, there is not much precedent for representing Exploding Kittens digitally. In fact, most instances of the game online are fan-made and often buggy, overly-complex, and/or excluding game features. Additionally, many of the existing implementations assume that at least one player will be a human, which may not necessarily be the case in our testing. What this means is that our best approach for this problem will likely begin with our own implementation of Exploding Kittens. <br>
The choice to forgo using an existing implementation of the game gives us the obvious benefit of being able to choose how we represent the game and its states. But, this in itself is also a challenge. How do we go about representing such a complex game? Obviously, there are reasons that existing implementations are often underwhelming; the game is not easy to recreate in its entirety. <br>
Once we’ve recreated the game to our satisfaction, another challenge becomes apparent. We’ve represented the game in its entirety, but where do we begin in representing a player’s information about the game-state? There are certainly obvious aspects of the state that the player will know such as their own hand, the size of the deck, the size of the hand of each opponent, etc. However, this does not begin to cover many of the minute details a player may be able to pick up. Say a player just re-inserted a card into the deck; they now know when that card will be drawn. Similarly, a player may have to give a card in their hand to one of their opponents; they now know one of the cards currently being held by said opponent. A lot of small actions can allow a player the smallest insights into the game-state. However, these small insights can still greatly alter how a player wants to behave during their next turn. This is the biggest challenge faced through this problem; Exploding Kittens is an extremely complex game where every small move can make information available to any player. This makes the game in itself extremely difficult to develop from the perspective of a player with an incomplete view of the game-state. In a similar vein, different information can affect how a player makes decisions. We will need to determine a way for the agents to consider their information and weigh their available options to maximize reward. That reward could be anything from survival, increasing the number of possible moves for the next turn, or ultimately winning the game. <br>
## Overview
Our general approach can be broken into a few stages. The first stage, and one of the main challenges, is representing the game. We decided for our version of Exploding Kittens to be zero player, meaning that there is no human interaction. This gave us the benefit of being able to quickly run vast amounts of games later on when training our agents. We also decided to create the game from scratch, which gave us the benefit of having access to all the information and the ability to modify our gamestate as necessary. <br>
The next stage of our project was creating some basic agents that could play the game against each other. We first created three of these agents, which we referred to as a random agent, a non-random agent, and a smart agent. The random agent is just that - random. It will make all of its decisions as if it was flipping a coin and did not consider any information about the gamestate. The second agent, our non-random agent, was built using some basic logic. This logic includes giving another player a less useful card when it has to give one up, and replanting an Exploding Kitten on top of the deck once after playing a Defuse card. All other decisions are made randomly. Finally, our smart agent attempts to add on to this by playing cards that affect the deck if the opponent has just replanted an Exploding Kitten. These cards are Attack, Skip, and Shuffle, all of which prevent the agent from directly drawing the top card. <br>
The final stage of our project entailed creating agents that used reward shaping and observing games between simpler agents to create a policy that was better than what our simpler agents were able to achieve. We created two agents that learned this way, but with different reward functions. The first agent was called our Survival Agent (SA), and it functioned by observing games between two simpler agents and assigning more reward to moves that resulted in the game lasting longer, and less reward to moves that resulted in the game being shorter. The idea is that if a game goes long, then both agents are making smarter moves than if the game did not last for a long time. The second agent that we created was called our Observed Policy Agent (OPA). This agent functioned similarly to the SA, however the reward was applied in a different way. For this agent, rewards were based around whether or not a move was made by an ultimately-winning agent or an ultimately-losing agent. <br>
For our SA and OPA we experimented with various hyper-parameters in training. We changed the number of observed games as well as the kinds of agents that were observed (between the three random or semi-random agents we had already built). Our biggest changes and what brought us the most variance in results was through the adjustments we made to our representation of the game-states and the knowledge-bases of our agents. <br>

# Related Work
## Stanford Team 
In 2018, a team of Stanford students attempted to use artificial intelligence to create agents that would play Exploding Kittens. Their implementation focused on two autonomous agents playing each other, and incorporated the rules that the original version of Exploding Kittens outlined. However, it’s worth noting that some card types were excluded to simplify the game state. The goal of this project for them was to see how well they can get agents to act under large amounts of uncertainty, and without existing knowledge or research to determine what optimal play would look like. To achieve this, they used two player self-play reinforcement learning, with a reward only given to the agent that does not explode by the end of the game. These two agents would play each other many times and be observed, although it is not specified exactly how many games were played during training. <br>
After observation, each agent’s experience is used to train a deep neural network, and the two agents are evaluated against each other. The best agent is then duplicated, and the process is repeated. In order to represent the process of playing a game, the team used a Markov Decision Process where a state is represented by the information available to a player at a given time, and an action is playing a certain card. Then, Monte Carlo Tree Search is used to estimate the optimal action for any given state. The neural network comes into play when MCTS reaches a leaf node. That node is then passed to the neural network to get the Q value of its edges. Overall, this approach was successful as the best agent created was able to win 100 games without losing against a random agent. <br>
## Microsoft’s chatbot Zo 
Microsoft’s chatbot named Zo was programmed to be able to play Exploding Kittens with its users in 2017. It features a GUI that makes the gamestate easy to understand and allows inexperienced users to take part in the game. With Zo’s millions of users it is feasible for the technology behind Zo’s ability to play to continue improving while it plays more games. There is not, however, much information about what this technology actually enails. The main goal of Zo being programmed to play Exploding Kittens is for users to have the option to play a singleplayer version of the game, and not necessarily create a perfectly optimal agent. <br>

# Approach
As mentioned earlier, our approach to this problem began with our own implementation of Exploding Kittens. We opted to recreate the game in Python, and designed our program to simulate games of Exploding Kittens rather than be a space where users can play Exploding Kittens. This allowed us to automate the simulation of an entire game with only computerized players. <br>
Our program has a class Game and a class Player, which can be abstracted to create agents with different policies. Games have a variable deck, which is a list of objects from our Card class. Each card is mapped to a different function that operates as the card being played. Defuse cards, which are automatically played, are played if the drawer has one and pulls an Exploding Kitten from the deck. Whenever an agent has to make a decision, there is a function within the Player class that executes the decision. <br>
Worth mentioning is that, during our implementation, we made the decision to restrict our testing to two-player games. This was for simplicity in representing the game and the observed game-states, and an extension of this project could be expanding our agents to function in games with more players. In a similar vein, we ultimately chose to exclude a handful of niche rules about the game, such as the protocol for playing three-of-a-kind cat cards. The Nope card was also removed from our implementation as the decision to play those cards is made out-of-turn and would have opened the project to complexities we did not have time to address. <br>
The default Player that we implemented acts entirely random. At the start of a turn they decide via coin-flip whether or not to play a card, and then the card played is also decided at random. When prompted to give a card to an opponent, a card is pulled from their hand at random. From here, we hoped to implement different agents with different strategies and policies by extending our Player class and overriding the decision functions. <br>
With there already being precedent in implementing MCTS and neural networks, we hoped to take our implementation of an Exploding Kittens artificial intelligence agent in a different direction. The directions that we were particularly interested in policy development. A primary motivation behind this is that it closely resembles how real players form strategy about the game. With such a complex game that features so many choices to make in a given turn, much of a player’s decision-making process may be formulated by experience and patterns. This is compounded by the fact that the player has such an incomplete understanding of the game-state; it’s hard to consider and develop a long-term holistic strategy that encompasses and accounts for all of the unknowns at play. Behind this motivation, we began by trying to implement an agent that can perform better than a random-choice agent by simply following a few ground-rule policies. <br>
After these agents were functioning, we looked to develop agents that were able to develop policy. We created two new kinds of agents, the SA and OPA. Each of these agents had a new method in their initialization that allowed them to be trained on a number of different games. At each turn, the agent would observe the game-state from the perspective of the moving agent. Then, it would note what move the agent is making. Once the game ends, the agent looks at all of the moves made by each agent and attributes value to them based on its priorities; the SA assigned value to moves that extended the length of the game while the OPA assigned value to moves that were made by the winning player. These values are stored in the agent’s memory, which was a dictionary of game-states and the attributed value of each possible move that was observed. Once enough games were observed to create a cohesive and complete policy, the policy-driven agents were tested against random agents. Their moves were selected based on which one yielded the highest return value; moves without precedent were decided randomly. <br>

# Evaluation, Results, & Testing
## Baseline Results 
In order to accurately judge the performance of our agents, we first needed to establish a baseline so see how well our random and logic-based agents perform. Our goal in creating these agents was not only to create autonomous agents to test out our policies, but also to have something for our policy based agents to compete against and judge their performance. This means that it is important for our random agents to win about 50% of the time against one another. Making sure this is balanced also ensures that factors like turn order do not affect our results. Our testing of these random and logic-based agents differs slightly than the testing of our other agents, as we do not need these agents to initially play a large number of games in the hopes of their policies improving. That means that we can immediately judge their results. We did this by having agents play 500 games against each other and recording the number of times each agent won. <br>
Our findings show that when two random agents play each other, the agent which goes first wins around 50% of the time. This is consistent with what we expected, as there is not a significant advantage in going first and each agent is otherwise playing the same. When a random agent plays a non-random agent, the non-random agent wins about 60% of the games. This shows that improvements can be made with a few basic rules, however more improvement is possible with better agents. We then had our smart agent play against a non-random agent. This smart agent was designed to build off of the logic used by a non-random agent, but also played defensively when it made sense to do so. When these two agents play each other, the smart agent wins about 55% of the time. Finally, we had our smart agent play a random agent. When this occurred, the smart agent won about 60-65% of the games. This shows that we can improve from a random agent by implementing logic that a relatively experienced player might use, and that we have an accurate baseline to evaluate our more advanced agents. <br>

## Advanced Agents Results 
Now that we have an understanding of how our basic agents perform, we can move on to evaluating our more advanced agents. We have two that we tested, our SA and OPA. Both of these work by observing a large amount of games between two other agents, and developing a policy that attempts to make the best moves based on different reward functions. The reward function for our SA was how many moves the agent survived, with the idea being if the game lasted a long time then the agent was likely making smart decisions. In our findings, this agent won about 65% of games when paired against a random agent. Interestingly enough, this performance did not change regardless of the kinds of random agents that the SA observed in training (random versus smart). We adjusted how many games this agent observed, and found that more games did not have a noticeable difference in how the agent performed at a certain point. This plateau became apparent at around 500,000 observed testing games. <br>
Generally speaking, it makes some intuitive sense that the SA would not fare significantly better than the agents it observes. It is hard to gauge the longevity of a game based upon single moves being made, as usually the pacing of the game is determined by how moves play out and are affected by one another. For example, the same move could be made in two different games: once at the start and once at the end. When the move is made at the end of the game, it will have a lower reward. This is in spite of whether or not the move in itself was responsible for the forthcoming end of the game. There is no real effective way to measure how individual moves affect the length of a game, and this lends itself to be an inconsistent and ineffective reward for policy agents in this game. <br>
The other agent that we tested, our OPA, fared better. We decided to have this agent observe 2,500,000 games, as we found increasing the number of observed games for this agent was actually beneficial. We also tested how our OPA performed when observing games between different agents. In our experience, this agent performed the best when it was trained on games between two smart agents, as opposed to random or non-random agents. At its optimal setup, our OPA won about 75% of the time against a random agent while playing a set of 500 games. This was our best performing agent, and was likely due to its reward function. <br>
The most interesting aspect of the OPA’s success was how much of it was attributed to what was represented in the agent’s observed game-state. Intuitively, it makes sense that having more information lends itself to better decisions. At its best setup, our agents were tracking the following:
The size of the deck
The cards in their current hand and specifically the number of Defuse cards in their hand
If the used a See The Future Card, whatever is at the top of the deck
The size of their opponent’s hand
Whether or not their opponent has a Defuse card
Whether an Attack card is active
	Even with all of these variables, other information about the game-state that would be potentially available to a player was not included due to implementation issues and timing. These include cards given to the opponent by players, the location of replanted Exploding Kittens, etc. In spite of this, we were able to see tangible improvement in our agents as we increased the information tracked and number of observed games.


Conclusions & Future Work
Broadly speaking, we were successful in our goals for this project. We wanted to see if it was feasible to create agents for Exploding Kittens that could consistently beat a random agent, and while our best agent could not win 100% of the games it played against a random agent, that was not our goal from the start. Exploding Kittens is a game that always has a large amount of uncertainty, and anyone who has played the game with their friends knows how much your success comes down to chance. However, creating an agent that wins 70-75% of the games it plays means that it was able to overcome a large amount of the chance that is inherent in the game. We also learned a lot about how autonomous agents can learn to navigate uncertainty which can be applied to future projects as there will almost always be some amount of uncertainty. 
Regarding future work, there are a few things that we can continue to work on to improve this project. First, we simplified the game to only have two players. While this makes the most sense when the goal is to learn about the methods used in artificial intelligence, it is not how the game is commonly played as there are normally 3+ players. We can see how our agents improve when observing games between three other agents, and basing their policy solely on the agent that wins. There is also the option to add more functionality to the game, such as the Nope card or extensions to the rules. The way our agents learn and play is independent of the specific rules of the game, making it relatively easy to add more functionality and see how our agents adapt. Finally, we can investigate different methods of learning other than reward shaping. We went with reward shaping in part because we had not seen that method in other attempts, but it is worth investigating to see how effective other methods could be. 
While we are generally satisfied with the progress we were able to make in our approach to this problem, we remain excited that there is much more work to be expanded upon.
Acknowledgements
Much of the inspiration of this project can be attributed to the work previously done by the team at Stanford University. Although our project attempted to move in a different direction, their project served as a motivating piece and an example to look up towards.
All coding was done using Python script editors. These were VSCode and ____. GitHub was also used to help store code and move it between computers.
Payton McAlice was responsible for the coded implementation of the game Exploding Kittens, as well as leading on the Observed Policy Agent.
Gabe Holmes was responsible for the random agent as well as the non-random and smart agents. He also led the work on the Survival Agent.
Exploding Kittens is a card game created by Elan Lee and Matt Inman.

Reference Section
2018, Radhika, et al. Reinforcement Learning for Exploding Kittens. Stanford University, 7 Dec. 2018.
“How to Play Exploding Kittens.” Exploding Kittens, https://www.explodingkittens.com/pages/rules-kittens. Accessed 4 May 2022.
“Microsoft’s Social Bot Zo Wants to Play Exploding Kittens with You - The AI Blog.” The AI Blog, https://www.facebook.com/microsoft, 13 Sept. 2017, https://blogs.microsoft.com/ai/microsofts-social-bot-zo-wants-play-exploding-kittens/.
Ng, Andrew, et al. Policy Invariance under Reward Transformations: Theory and Application to Reward Shaping. University of California, Berkeley, 1999. Accessed 4 May 2022.

Appendices
Code will be attached via .zip file
Link to GitHub repository: https://github.com/gabeh33/kittens 

